{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-nagar/cs4372/blob/main/transformers_using_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK14EIYE81F6"
      },
      "source": [
        "The code below attempts to create a transformer model using Keras layers and demonstrates an application for English-to-Spanish translation. Code is adapted from the book [Deep Learning with Python, Third Edition](https://www.manning.com/books/deep-learning-with-python-third-edition). The book's contents are available online at [deeplearningwithpython.io](https://deeplearningwithpython.io)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVZaiEiV81F8",
        "outputId": "1961b910-4d6a-42a4-e794-445ca21900b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m1.4/1.5 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keras keras-hub==0.21.1 --upgrade -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zgOH0szp81F9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B06XcQ-I81F9"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "from IPython.core.magic import register_cell_magic\n",
        "\n",
        "@register_cell_magic\n",
        "def backend(line, cell):\n",
        "    current, required = os.environ.get(\"KERAS_BACKEND\", \"\"), line.split()[-1]\n",
        "    if current == required:\n",
        "        get_ipython().run_cell(cell)\n",
        "    else:\n",
        "        print(\n",
        "            f\"This cell requires the {required} backend. To run it, change KERAS_BACKEND to \"\n",
        "            f\"\\\"{required}\\\" at the top of the notebook, restart the runtime, and rerun the notebook.\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIAtEQHY81F9"
      },
      "source": [
        "## Language models and the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1yW1Ukh81F9"
      },
      "source": [
        "### The language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iSxEiG1L81F-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "import keras\n",
        "\n",
        "sequence_length = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FPD6b8N781F-"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 256\n",
        "hidden_dim = 1024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx79DqCb81F_"
      },
      "source": [
        "#### English-to-Spanish translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JrZUu0oy81F_"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "zip_path = keras.utils.get_file(\n",
        "    origin=(\n",
        "        \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "    ),\n",
        "    fname=\"spa-eng\",\n",
        "    extract=True,\n",
        ")\n",
        "text_path = pathlib.Path(zip_path) / \"spa-eng\" / \"spa.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XzjSrV1C81GA"
      },
      "outputs": [],
      "source": [
        "with open(text_path) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, spanish = line.split(\"\\t\")\n",
        "    spanish = \"[start] \" + spanish + \" [end]\"\n",
        "    text_pairs.append((english, spanish))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAV9MQY181GA",
        "outputId": "e854a2f1-a2b3-4d38-fec9-0752d1ed079f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Industrialization had a great influence on the development of the economy in Japan.',\n",
              " '[start] La industrialización ejerció gran influencia en el desarrollo económico japonés. [end]')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import random\n",
        "random.choice(text_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IRd6qAjz81GA"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(text_pairs)\n",
        "val_samples = int(0.15 * len(text_pairs))\n",
        "train_samples = len(text_pairs) - 2 * val_samples\n",
        "train_pairs = text_pairs[:train_samples]\n",
        "val_pairs = text_pairs[train_samples : train_samples + val_samples]\n",
        "test_pairs = text_pairs[train_samples + val_samples :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "457Ic8sV81GA"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\"\n",
        "    )\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "english_tokenizer = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "spanish_tokenizer = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "english_tokenizer.adapt(train_english_texts)\n",
        "spanish_tokenizer.adapt(train_spanish_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m1nKeJwL81GA"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = english_tokenizer(eng)\n",
        "    spa = spanish_tokenizer(spa)\n",
        "    features = {\"english\": eng, \"spanish\": spa[:, :-1]}\n",
        "    labels = spa[:, 1:]\n",
        "    sample_weights = labels != 0\n",
        "    return features, labels, sample_weights\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpfPL4a581GA",
        "outputId": "fa07f3aa-b97b-46a0-8888-4022eb86bc5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 20)\n"
          ]
        }
      ],
      "source": [
        "inputs, targets, sample_weights = next(iter(train_ds))\n",
        "print(inputs[\"english\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBVjQ7Hb81GA",
        "outputId": "d17d2929-30a7-4e1e-9d00-c7ad69c1ec31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 20)\n"
          ]
        }
      ],
      "source": [
        "print(inputs[\"spanish\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGID9rtC81GA",
        "outputId": "69be280e-1e37-4f0d-9ead-bad588eadf70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 20)\n"
          ]
        }
      ],
      "source": [
        "print(targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXAPZcbq81GA",
        "outputId": "3115590e-7fe8-4c92-b897-7a06c70c4b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 20)\n"
          ]
        }
      ],
      "source": [
        "print(sample_weights.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zqAaQWdf81GA"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "hidden_dim = 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lQHA2m_781GB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "spa_vocab = spanish_tokenizer.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7axd2Sc81GB"
      },
      "source": [
        "### The Transformer architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SJzQmbJ81GB"
      },
      "source": [
        "#### Dot-product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCdxCXNZ81GB"
      },
      "source": [
        "#### Transformer encoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cNPVUYHq81GB"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(keras.Layer):\n",
        "    def __init__(self, hidden_dim, intermediate_dim, num_heads):\n",
        "        super().__init__()\n",
        "        key_dim = hidden_dim // num_heads\n",
        "        self.self_attention = layers.MultiHeadAttention(num_heads, key_dim)\n",
        "        self.self_attention_layernorm = layers.LayerNormalization()\n",
        "        self.feed_forward_1 = layers.Dense(intermediate_dim, activation=\"relu\")\n",
        "        self.feed_forward_2 = layers.Dense(hidden_dim)\n",
        "        self.feed_forward_layernorm = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, source, source_mask):\n",
        "        residual = x = source\n",
        "        mask = source_mask[:, None, :]\n",
        "        x = self.self_attention(query=x, key=x, value=x, attention_mask=mask)\n",
        "        x = x + residual\n",
        "        x = self.self_attention_layernorm(x)\n",
        "        residual = x\n",
        "        x = self.feed_forward_1(x)\n",
        "        x = self.feed_forward_2(x)\n",
        "        x = x + residual\n",
        "        x = self.feed_forward_layernorm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLrCSKR181GB"
      },
      "source": [
        "#### Transformer decoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cGn5A_H681GB"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(keras.Layer):\n",
        "    def __init__(self, hidden_dim, intermediate_dim, num_heads):\n",
        "        super().__init__()\n",
        "        key_dim = hidden_dim // num_heads\n",
        "        self.self_attention = layers.MultiHeadAttention(num_heads, key_dim)\n",
        "        self.self_attention_layernorm = layers.LayerNormalization()\n",
        "        self.cross_attention = layers.MultiHeadAttention(num_heads, key_dim)\n",
        "        self.cross_attention_layernorm = layers.LayerNormalization()\n",
        "        self.feed_forward_1 = layers.Dense(intermediate_dim, activation=\"relu\")\n",
        "        self.feed_forward_2 = layers.Dense(hidden_dim)\n",
        "        self.feed_forward_layernorm = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, target, source, source_mask):\n",
        "        residual = x = target\n",
        "        x = self.self_attention(query=x, key=x, value=x, use_causal_mask=True)\n",
        "        x = x + residual\n",
        "        x = self.self_attention_layernorm(x)\n",
        "        residual = x\n",
        "        mask = source_mask[:, None, :]\n",
        "        x = self.cross_attention(\n",
        "            query=x, key=source, value=source, attention_mask=mask\n",
        "        )\n",
        "        x = x + residual\n",
        "        x = self.cross_attention_layernorm(x)\n",
        "        residual = x\n",
        "        x = self.feed_forward_1(x)\n",
        "        x = self.feed_forward_2(x)\n",
        "        x = x + residual\n",
        "        x = self.feed_forward_layernorm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6uZVjrI81GB"
      },
      "source": [
        "#### Sequence-to-sequence learning with a Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "C3pHBr6R81GB"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 256\n",
        "intermediate_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int32\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, hidden_dim)(source)\n",
        "encoder_output = TransformerEncoder(hidden_dim, intermediate_dim, num_heads)(\n",
        "    source=x,\n",
        "    source_mask=source != 0,\n",
        ")\n",
        "\n",
        "target = keras.Input(shape=(None,), dtype=\"int32\", name=\"spanish\")\n",
        "x = layers.Embedding(vocab_size, hidden_dim)(target)\n",
        "x = TransformerDecoder(hidden_dim, intermediate_dim, num_heads)(\n",
        "    target=x,\n",
        "    source=encoder_output,\n",
        "    source_mask=source != 0,\n",
        ")\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_predictions = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([source, target], target_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "iztn-PEg81GB",
        "outputId": "cc83de9d-2246-4416-cc58-b7075d82cf1e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to      \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ english (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │           \u001b[38;5;34m0\u001b[0m │ -                  │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │   \u001b[38;5;34m3,840,000\u001b[0m │ english[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │           \u001b[38;5;34m0\u001b[0m │ english[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ spanish (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │           \u001b[38;5;34m0\u001b[0m │ -                  │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ transformer_encoder   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │   \u001b[38;5;34m1,315,072\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)  │                   │             │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ not_equal_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │           \u001b[38;5;34m0\u001b[0m │ english[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)            │                   │             │                    │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ embedding_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │   \u001b[38;5;34m3,840,000\u001b[0m │ spanish[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)           │                   │             │                    │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ transformer_decoder   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │   \u001b[38;5;34m1,578,752\u001b[0m │ transformer_encod… │\n",
              "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)  │                   │             │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│                       │                   │             │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │           \u001b[38;5;34m0\u001b[0m │ transformer_decod… │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │   \u001b[38;5;34m3,855,000\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│                       │ \u001b[38;5;34m15000\u001b[0m)            │             │                    │\n",
              "└───────────────────────┴───────────────────┴─────────────┴────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)          </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">     Param # </span>┃<span style=\"font-weight: bold\"> Connected to       </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ english (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                  │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │   <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ english[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ english[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ spanish (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                  │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ transformer_encoder   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │   <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,072</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)  │                   │             │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ not_equal_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ english[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)            │                   │             │                    │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ embedding_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │   <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ spanish[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │                   │             │                    │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ transformer_decoder   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │   <span style=\"color: #00af00; text-decoration-color: #00af00\">1,578,752</span> │ transformer_encod… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)  │                   │             │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│                       │                   │             │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_decod… │\n",
              "├───────────────────────┼───────────────────┼─────────────┼────────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">3,855,000</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│                       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            │             │                    │\n",
              "└───────────────────────┴───────────────────┴─────────────┴────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,428,824\u001b[0m (55.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,428,824</span> (55.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,428,824\u001b[0m (55.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,428,824</span> (55.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "transformer.summary(line_length=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJsfWKp581GB",
        "outputId": "7f32e7f6-0d38-4443-e22d-13982e754d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 27ms/step - accuracy: 0.3589 - loss: 1.4880 - val_accuracy: 0.4923 - val_loss: 1.0469\n",
            "Epoch 2/5\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 20ms/step - accuracy: 0.5287 - loss: 0.9783 - val_accuracy: 0.5689 - val_loss: 0.8396\n",
            "Epoch 3/5\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 21ms/step - accuracy: 0.6004 - loss: 0.7673 - val_accuracy: 0.5968 - val_loss: 0.7655\n",
            "Epoch 4/5\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 22ms/step - accuracy: 0.6459 - loss: 0.6400 - val_accuracy: 0.6095 - val_loss: 0.7348\n",
            "Epoch 5/5\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 22ms/step - accuracy: 0.6782 - loss: 0.5536 - val_accuracy: 0.6186 - val_loss: 0.7270\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b4dd6959910>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "transformer.fit(train_ds, epochs=5, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPbbU4de81GB"
      },
      "source": [
        "#### Embedding positional information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TYbt8czV81GB"
      },
      "outputs": [],
      "source": [
        "from keras import ops\n",
        "\n",
        "class PositionalEmbedding(keras.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = layers.Embedding(input_dim, output_dim)\n",
        "        self.position_embeddings = layers.Embedding(sequence_length, output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        positions = ops.cumsum(ops.ones_like(inputs), axis=-1) - 1\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mzo7uj2B81GB"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 256\n",
        "intermediate_dim = 2056\n",
        "num_heads = 8\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int32\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, hidden_dim)(source)\n",
        "encoder_output = TransformerEncoder(hidden_dim, intermediate_dim, num_heads)(\n",
        "    source=x,\n",
        "    source_mask=source != 0,\n",
        ")\n",
        "\n",
        "target = keras.Input(shape=(None,), dtype=\"int32\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, hidden_dim)(target)\n",
        "x = TransformerDecoder(hidden_dim, intermediate_dim, num_heads)(\n",
        "    target=x,\n",
        "    source=encoder_output,\n",
        "    source_mask=source != 0,\n",
        ")\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_predictions = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([source, target], target_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_YMFJ_l81GC",
        "outputId": "de0c8395-6ac1-45ec-d184-f1d89e0d6c5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 28ms/step - accuracy: 0.3676 - loss: 1.4649 - val_accuracy: 0.5108 - val_loss: 1.0080\n",
            "Epoch 2/5\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 22ms/step - accuracy: 0.5550 - loss: 0.9275 - val_accuracy: 0.6037 - val_loss: 0.7832\n",
            "Epoch 3/5\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 22ms/step - accuracy: 0.6313 - loss: 0.7210 - val_accuracy: 0.6362 - val_loss: 0.7014\n",
            "Epoch 4/5\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 22ms/step - accuracy: 0.6737 - loss: 0.6022 - val_accuracy: 0.6485 - val_loss: 0.6676\n",
            "Epoch 5/5\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 22ms/step - accuracy: 0.7024 - loss: 0.5228 - val_accuracy: 0.6622 - val_loss: 0.6450\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b4d8cf6a4e0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "transformer.fit(train_ds, epochs=5, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5HLycfR81GC",
        "outputId": "ef7dbb8f-db30-443d-afad-44df3dfc8aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "I can't let them catch you.\n",
            "[start] no puedo dejar que te [UNK] [end]\n",
            "-\n",
            "I found the key I was looking for.\n",
            "[start] encontré la llave de la clave de mirar [end]\n",
            "-\n",
            "She went for a walk.\n",
            "[start] ella salió a caminar [end]\n",
            "-\n",
            "There is no money in my bag.\n",
            "[start] no hay dinero en mi bolsa [end]\n",
            "-\n",
            "What did you spill?\n",
            "[start] qué hiciste [end]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "spa_vocab = spanish_tokenizer.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "\n",
        "def generate_translation(input_sentence):\n",
        "    tokenized_input_sentence = english_tokenizer([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(sequence_length):\n",
        "        tokenized_target_sentence = spanish_tokenizer([decoded_sentence])\n",
        "        tokenized_target_sentence = tokenized_target_sentence[:, :-1]\n",
        "        inputs = [tokenized_input_sentence, tokenized_target_sentence]\n",
        "        next_token_predictions = transformer.predict(inputs, verbose=0)\n",
        "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(5):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(generate_translation(input_sentence))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}